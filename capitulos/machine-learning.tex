\chapter{Aprendizado de Máquina}

Como subárea da inteligência artificial, o aprendizado de máquina tem como objetivo criar técnicas computacionais e sistemas que automaticamente adquirem conhecimento. Existem diversos algoritmos de aprendizado de máquina, utilizados para resolver problemas específicos, portanto, é importante compreender suas limitações \cite{rezende2003}.

As tarefas de aprendizado de máquina podem ser classificadas em três categorias \cite{russel2003}:

\begin{itemize}
    \item \textbf{Apredizagem supervisionada:} é fornecido ao algoritmo um conjunto de entradas e suas respectivas saídas, com o objetivo de aprender uma regra geral que mapeia as entradas às saídas.
    \item \textbf{Aprendizagem não-supervisionada:} nenhum tipo de entrada é fornecido, com o objetivo do próprio algoritmo identificar os padrões do conjunto de dados.
    \item \textbf{Aprendizagem por reforço:} o algoritmo interage com o ambiente dinâmico afim de concluir determinados objetivos.
\end{itemize}


\section{Aprendizagem Supervisionada}

Na aprendizagem supervisionada, cada exemplo de treinamento é descrito por um conjunto de atributos que servem como dados de entrada e são associados a um valor de saída. A partir de um conjunto pré-definido de entradas e saídas, o algoritmo consegue gerar uma saída adequada para uma nova entrada. A aprendizagem supervisionada é a principal técnica utilizada para casos de classificação \cite{mohri2012}.

\subsection{Aprendizado Bayesiano}

O aprendizado Bayesiano é do tipo supervisionado, já que recebe como \textit{input} dados iniciais e seus respectivos rótulos (ou classes). O algoritmo utiliza fórmulas estatísticas e cálculos de probabilidades para realizar a classificação \cite{mitchell1997}.

As principais vantagens do aprendizado bayesiano são: o fato de se poder embutir nas probabilidades calculadas o conhecimento de domínio que se tem (caso se tenha) e a capacidade das classificações feitas pelo algoritmo se basearem em evidências fornecidas. Por outro lado, muitas probabilidades devem ser calculadas, o que pode ocasionar em um alto custo computacional. Uma das soluções para o custo do cálculo das probabilidades é o uso do classificador bayes ingênuo \cite{pardo2002}.

\subsubsection{Classificador \textit{naive bayes}}

O classificador bayesiano, também chamado de bayes ingênuo (ou \textit{naive bayes}, em inglês), admite que os atributos do elemento a ser classificado são independentes entre si, mesmo que eles possuam tal dependência \cite{pellucci2011}.

Segundo \citeonline{oguri2007}, existem basicamente dois tipos de classificadores bayesianos ingênuos: o modelo binário e o modelo multinomial. O modelo binário representa um documento como um vetor binário, ou seja, o valor 0 em uma posição \(k\) (onde \(k\) representa uma palavra do documento) representa a não ocorrência do termo e o valor 1 representa ao meno uma ocorrência desse termo. Já o modelo multinomial assume que o documento é representado por um vetor de inteiros, representando a quantidade de vezes que um termo ocorre no documento. Entretanto, quando lidamos com dados contínuos, os seus valores são distribuídos de acordo com uma distribuição Gaussiana \cite{hand2001}.

O classificador \textit{naive bayes} é baseado na aplicação do Teorema de Bayes:
%
\begin{align}
P(classe|A) = \frac{P(A|classe) \times P(classe)}{P(A)},
\end{align}
%
onde:

\begin{itemize}
    \item \(P(classe)\) é a probabilidade \textit{a priori} da classe em questão,
    \item \(P(A)\) é a probabilidade \textit{a priori} da nova instância a ser classificada,
    \item \(P(A|classe)\) é a probabilidade \textit{a posteriori} de \(A\) condicional a \(classe\) e
    \item \(P(classe|A)\) é a probabilidade \textit{a posteriori} de \(classe\) condicional a \(A\).
\end{itemize}

Para calcular a classe mais provável da nova instância, calcula-se as probabilidades de todas as classes possíveis e escolhe-se a classe com maior probabilidade. Em termos estatísticos, isso é equivalente a maximizar \(P(classe|A)\), ou seja, maximiza-se o valor do numerador e minimíza-se o valor do denominador. Como o denominador não depende da variável \(classe\), pode ser ignorado. Temos então:
%
\begin{align}
P(classe|A) = P(A|classe) \times P(classe)
\end{align}

Considerando que \(A= ( a_1, a_2, \ldots , a_n)\), onde \(a_n\) são os atributos que compõe \(A\), a suposição ``ingênua'' que o classificador faz é que todos os atributos de \(A\) são independentes entre si, o que simplifica o cálculo da probabilidade de \(P(A|classe)\), podendo ser reduzida a \(P(a_1|classe) \times P(a_2|classe) \times\ \ldots \times P(a_n|classe)\). Logo,
%
\begin{align}
P(classe|A) = \prod_{i=1}^{n} P(a_i|classe) \times P(classe)
\end{align}

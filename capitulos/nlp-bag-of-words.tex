\clearpage
\chapter{Processamento de Linguagem Natural}

Este capítulo contém o referencial teórico que diz respeito ao Processamento de Linguagem Natural.

\section{Pré-processamento: modelo \textit{bag-of-words}}
\label{sec:modelo_te}

A grande parte dos dados que serão utilizados nesse trabalho estão dispostos em formato de texto, ou seja, um formato não estruturado que dificulta a extração de informações. Um método comum do processamento de texto em linguagem natural é o modelo \textit{bag-of-words}, onde o texto é representado na forma de um vetor de frequência de palavras que ocorrem no texto \cite{pretext}.

A representação computacional de um texto no modelo mencionado é feita através de um dicionário onde suas chaves são os termos presentes no documento e os valores são suas respectivas frequências. Tomando o trecho ``fui à padaria e comprei pão'' como exemplo a sua representação no modelo \textit{bag-of-words} corresponde a:

\begin{lstlisting}[language=Python, caption=Representação de um trecho  no modelo bag-of-words]
bag_of_words = {
    "fui": 1,
    "à": 1,
    "padaria": 1,
    "e": 1,
    "comprei": 1,
    "pão": 1,
}
\end{lstlisting}

É fácil ver que esta representação torna a ordem das palavras irrelevante. As frases ``e comprei fui padaria pão à'' e ``à pão comprei padaria e fui'' também possuem as mesmas representações.

Um vetor também pode ser uma forma de representar um texto. Nesse caso, cada índice do vetor representa uma palavra e cada elemento corresponde à frequência da mesma.

A linguagem de programação \textit{Python} possui, nativamente, ferramentas que facilitam a contagem de elementos de um dicionário. O \texttt{Counter} \footnote{https://docs.python.org/3/library/collections.html} é uma subclasse de \texttt{dict} e assim como a classe pai, é uma coleção não ordenada de elementos. Os elementos são armazenados como chaves de dicionário e seus contadores como valores, que podem assumir qualquer valor inteiro, incluindo zero e negativos. Isso torna o \texttt{Counter} uma representação ideal para o modelo \textit{bag-of-words}, já que ele, naturalmente, conta a quantidade de vezes que um termo aparece dentro de uma coleção (uma lista, tupla ou dicionário, por exemplo).



\subsection{Modelo \textit{N-Gram}}

O modelo de representação de textos através de um conjunto de palavras pode limitar qualitativamente a análise que será realizada, já que frases não são consideradas. Por exemplo, ``Santa Catarina'', um estado brasileiro, possui um significado completamente diferente quando as palavaras ``Santa'' (mulher canonizada), e ``Catarina'' (nome feminino) são analisadas separadamente.

Um \(n\)-grama é uma sequência de \(n\) elementos dentro de um texto. Os elementos podem ser palavras, sílabas, letras ou qualquer outra base. Um \(n\)-grama de tamanho 1 é chamado de unigrama, de tamanho 2, bigrama, e de tamanho 3, trigrama. Sequências com 4 ou mais elementos são chamados de \(n\)-gramas. Usando a frase ``Eu não gostei desse filme'' como exemplo, temos os seguintes unigramas: ``eu'', ``não'', ``gostei'', ``desse'' e ``filme''. Os seguintes bigramas: ``eu não'', ``não gostei'', ``gostei desse'' e ``desse filme''. E os seguintes trigramas: ``eu não gostei'', ``não gostei desse'' e ``gostei desse filme''.

\subsection{Representação dos Termos}
\label{sec:representação_dos_termos}

A representação de um termo \(a_i\) pode se dar de diferentes maneiras, como a quantidade de vezes que o termo aparece em um texto ou apenas se o termo aparece no texto, por exemplo. Alguns dos tipos de representação serão expostos a seguir.

\subsubsection{\textit{Boolean}}
\label{ssub:bag-boolean}

Essa medida usa a representação binária para os termos presentes nos documentos, onde o valor de \(a_{i}\) é 0 quando o termo não aparece nenhuma vez no documento ou 1 quando aparece uma ou mais vezes. Essa medida é muito simples e, geralmente, modelos estatísticos levam em consideração também a frequência com que os termos se repetem nos documentos \cite{buckley1988}.

\subsubsection{\textit{Term Frequency}}
\label{ssub:baf-tf}

Ao contrário da medida mencionada anteriormente, a medida \textit{term frequency} considera a quantidade de ocorrencias do termo \(t\) dentro do documento
%
\begin{equation}
f_t=\frac{n_t}{N},
\end{equation}
%
onde \(n_t\) é a quantidade de vezes que o termo \(t\) aparece dentro do documento e \(N\) a quantidade total de termos do documento.

Alguns termos comuns podem aparecer na maioria dos documentos sem fornecer informações úteis em uma tarefa de mineração de textos \cite{pretext}.

\subsubsection{\textit{Term Frequency - Inverse Document Frequency}}
\label{ssub:baf-tfidf}

Para diminuir a influência de termos comuns, é possível utilizar um fator de ponderação, para que os termos que aparecem na maioria dos documentos tenham valores numéricos menores do que aqueles que raramente aparecem \cite{pretext}. Segundo \citeonline{jones1972}, a especificidade de um termo pode ser quantificada por uma função inversa do número de documentos em que ele ocorre, essa função varia entre \(0\) e \(log N_d\), onde \(N_d\) é o número total de documentos e \(d(t)\) a quantidade de documentos nos quais o termo \(t\) aparece ao menos uma vez:
%
\begin{equation}
i(t)=log \frac{N_d}{d(t)}
\end{equation}
%
Portanto, o valor final de \(a_{i}\) é dado pela equação:
%
\begin{equation}
f(t)=\frac{n_t}{N} \cdot log \frac{N_d}{d(t)},
\end{equation}
%
onde \(\frac{n_t}{N}\) é a frequência do termo dentro do texto e \(log \frac{N_d}{d(t)}\) sua taxa de ponderação.

\subsection{Dimensionalidade dos documentos}
\label{sub:dimensionalidade_dos_documentos}

A representação vetorial de uma coleção de documentos no modelo \textit{bag-of-words} pressupõe um espaço dimensional igual ao número de termos presentes em toda a coleção de documentos. Suponha que analisou-se 10 documentos e, em média, foram retirados 200 novos termos de cada um. Logo, a dimensionalidade média dos vetores serão de, aproximadamente, 2000. Se a maior parte dos termos aparecer em apenas um ou dois documentos, teremos a maior parte das componentes vetoriais nula. \cite{pretext}. Existem métodos cujo objetivo é diminuir a dimensionalidade desses vetores. Dentre eles, citamos a transformação de cada termo no radical de origem, utilizando algoritmos de \textit{stemming}.

\subsubsection{Stemização}
\label{ssub:stemização}

A stemização (do inglês, \textit{stemming}) é o processo de reduzir palavras flexionadas à sua raiz, essa redução não precisa, necessariamente, chegar à raiz morfológica da palavra. A raiz obtidada geralmente é o suficiente para mapear palavras relacionadas à um valor comum, mesmo se este não for uma raiz válida. O estudo de algoritmos de \textit{stemming} é foco de pesquisas desde a década de 60 e o primeiro algoritmo foi publicado por \citeonline{lovins1968}.

A consequência da aplicação de algoritmos de \textit{stemming} consiste na remoção de prefixos ou sufixos de um termo e ou da transformação de verbos para suas formas no infinitivo. Por exemplo, as palavras \textbf{ouvir}, \textbf{ouvi}, \textbf{ouviriam}, \textbf{ouve} e \textbf{ouvindo} seriam reduzidas para um mesmo \textit{stem}: \textbf{ouv}. Esse método diminui, portanto, a dimensionalidade dos vetores e dicionários dentro de uma \textit{bag-of-words}. Ao invés de analizar a frequência dos termos, analisamos a quantidade de vezes que um \textit{stem} aparece em um documento.

É evidente que os algoritmos de \textit{stemming} são dependentes do idioma analisado. O algoritmo de \citeonline{porter1980}, um dos algoritmos de \textit{stemming} mais conhecidos, remove os sufixos de termos em inglês, tem sido amplamente utilizado, referenciado e adaptado desde sua criação. É possível adaptá-lo para a língua portuguesa considerando que as línguas provenientes do latim possuem formas verbais conjugadas em sete tempos e com sete terminações distintas.

Devido ao fato de uma linguagem ter tantas regras e exceções, é pouco provavél que o algoritmo de \textit{stemming} retorne o mesmo \textit{stem} para todas as palavras que tenham a mesma origem ou radical morfológico. Pode-se dizer, também, que a medida que o algoritmo vai se tornando específico o suficiente para atender todas essas regras e exceções a eficiência do algoritmo também diminui \cite{imamura2001}.

\subsubsection{\textit{Stop Words}}
\label{ssub:stop_words}

Palavras que possuem pouco ou nenhum valor semântico, como \textit{"e"}, \textit{"de"} e \textit{"seus"}, são conhecidas como \textit{Stop Words} e, por não agregarem valor à analise textual, são removidas durante o pré-processamento \cite{rajaraman2011}. Essas palavras não são exclusividade de uma linguagem específica e geralmente representam a maioria dos termos de um texto. No caso da língua inglesa, por exemplo, palavras como \textit{"of"} e \textit{"the"} também não possuem nenhum valor para a análise. A lista de \textit{Stop Words} obviamente varia de acordo com a linguagem que está sendo analisada \cite{lopes2015}. O quadro abaixo mostra a lista de \textit{Stop Words} consideradas neste trabalho.

\begin{table}[h]
\centering
\begin{tabular}{cccccccc}
\cline{1-8}
de & os & tua & tem & estão & da & lhes & essas \\
e & é & foi & nossas & muito & o & se & tuas \\
tu & por & as & sua & aquele & entre & não & ele \\
delas & minhas & às & nos & pela & havia & me & como \\
ser & aqueles & nossa & vocês & eu & ter & tenho & suas \\
está & isso & pelos & estes & tinha & depois & foram & este \\
para & só & quem & deles & isto & um & eles & do \\
vos & mais & mesmo & num & dele & será & minha & a \\
no & teus & à & você & em & meus & esses & pelas \\
com & ao & dela & há & que & na & nosso & te \\
aos & dos & ou & aquela & era & uma & das & esta \\
teu & nem & já & até & seja & esse & mas & quando \\
aquelas & nossos & têm & também & seus & lhe & meu & seu \\
ela & elas & estas & nós & sem & essa & fosse & qual \\
& & pelo & nas & numa & aquilo & & \\
\cline{1-8}
\end{tabular}
\caption{Exemplos de \textit{Stop Words}}
\label{exemplos-stop-words}
\end{table}

